<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://bastibe.de/rss.xml"
      title="RSS feed for https://bastibe.de/">
<title>Bastibe.de</title>
<meta name="author" content="Bastian Bechtold">
<meta name="referrer" content="no-referrer">
<link href= "static/style.css" rel="stylesheet" type="text/css" />
<link rel="icon" href="static/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="static/favicon-152.png">
<link rel="msapplication-TitleImage" href="static/favicon-144.png">
<link rel="msapplication-TitleColor" href="#0141ff">
<script src="static/katex.min.js"></script>
<script src="static/auto-render.min.js"></script>
<link rel="stylesheet" href="static/katex.min.css">
<script>document.addEventListener("DOMContentLoaded", function() { renderMathInElement(document.body); });</script>
<meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8">
<meta name="viewport" content="initial-scale=1,width=device-width,minimum-scale=1"></head>
<body>
<div id="preamble" class="status"><div class="header">
  <a href="https://bastibe.de">Basti's Scratchpad on the Internet</a>
  <div class="sitelinks">
    <a href="https://twitter.com/paperflyer">Twitter</a> | <a href="https://github.com/bastibe">Github</a> | <a href="https://bastibe.de/projects.html">Projects</a>
  </div>
</div></div>
<div id="content">

<div class="post-date">27 Dec 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-12-27-ios-and-android.html">A review of iOS, from an Android user's perspective</a></h1>
<p>
My Pixel 2 phone received its final software update the other day, and its battery took the occasion to really show its age. So it was time for a new phone. Which is not a decision to make lightly, considering that I spend multiple hours per day using it. And this time, I was inclined to give the iPhone another go, mostly because of Google's gross lack of human decency in the last few years. So, after years of using various Android devices, I bought a used, red, iPhone SE (2020). I made this choice with some trepidations, as my last experience with iOS was an iPad 3 from 2012. These are my experiences.
</p>

<p>
As a seasoned Android user, my first impressions of iOS were a bit of a mess. There are things that swipe up from the bottom, swipe down from the top, to the left of the home screens, to the right of the home screens, and at various permutations of pressing/holding the home button. Everything animates, takes a long time, and moves the viewport around. Particularly annoying is the placement of the back button in the top left corner, i.e. the most-inconvenient place on the entire screen for the arguably most-important gesture of the entire UI<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>. A close second are the context menus that slide out from a long-pressed item, with the menu thus in a different position on the screen every time you long press anything. These things may be less flashy on Android, but are seriously simpler.
</p>

<p>
I also immediately missed Android's customizeable home screen, with freely-positionable app icons and a plethora of useful widgets. iOS is very restrictive in this regard, and seemingly for no good reason. Why is the list of all apps (right-of-homescreen) sorted arbitrarily into nonsensical folders instead of a plain list? Why are app widgets allowed, but only on that weird left-of-the-home-screen screen? Why can't I have a currently-playing widget for my podcast player, a weather radar, or my groceries list? Apparently, iOS 14 has a brand new API that does now allow Android-like widgets in iOS, but at the moment they were only available for Apple's own (useless) apps.
</p>

<p>
My second big stumbling block was the iOS on-screen keyboard. At first glance, I thought it a terribly clunky thing. Actions as simple as inserting a comma require multiple taps, and positioning the cursor seemed almost comically difficult. But then I discovered that the "123" button in the bottom left can be swiped instead of tapped, which makes commas and periods and hyphens available to a quick swipe. That is seriously cool, if slightly hampered by my accidentally activating Control Center instead of swiping from "123" a bit too often. And precise cursor positioning is hidden behind a long-press of the spacebar. Very cool indeed. With these gestures, the iOS on-screen keyboard is actually not bad at all.
</p>

<p>
Autocorrect seems capable as well, and multi-language aware (hear that, Android?). And, mind-blowingly, a triple-swipe on the content area engages undo and redo in the current text area. Albeit a nigh-undiscoverable gesture, this is miles better than Androids undo/redo system (there isn't one). Actually, Android text fields <i>do</i> support undo and redo if you press Ctrl-Z on the keyboard, it's just that no on-screen keyboard has a Ctrl key. This is my number one grievance with Android at the moment (although there are <a href="https://play.google.com/store/apps/details?id=com.catchingnow.undo&amp;hl=en_US&amp;gl=US">workarounds</a>).
</p>

<p>
As a summary to the keyboard situation, I came to respect the iOS keyboard. I still miss a German "ß" key, more control about the autocorrect system, and a more modern design. But it works reasonably well. Better than Android's stock keyboard for sure.
</p>

<p>
My second big hurdle with iOS was the camera. Just like my Pixel's camera, the iPhone takes perfectly acceptible photos. But everything else is just plain worse than on Android. It starts with the way you get access to the camera, either by swiping right on the lock screen, or by tapping the camera app on the home screen. On Android, you double-click the lock button to start the camera, then hit the volume button to take a photo. Not only is this significantly faster, it also requires no ungloved finger, and no look onto the screen. And it doesn't make that abominable shutter sound, either, that the iPhone requires (unless the entire phone is silenced or you take video).
</p>

<p>
When I take a picture with my phone, it is because I don't have a dedicated camera at hand. Considering the number of cameras I own, this is most likely to happen when I need a camera <i>fast</i>. Thus the speed from pants to picture is extremely important to me, and this is a clear victory for Android. And additionally, Pixel phones have supported stacked RAWs since the original Pixel<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>, and offer quite a comprehensive image editing suite right in the stock camera app. And I prefer the Pixel's relatively neutral rendering over the iPhone's oversharpened, over-denoised, waxy images. But that might be just me.
</p>

<p>
At this point, I had more or less given up on iOS, and bought a Pixel 4a, which is probably Android's closest competitor to the iPhone SE.
</p>

<p>
Beyond the camera, there were a number of annoyances with iOS that I found hard to get used to. Like the fingerprint reader on the SE being very unreliable for me (50% miss rate), and awkwardly requiring a distinct push to activate the home button, where the Pixel's is quicker and in a more convenient location. The home button is in fact not a button, but an immovable touch pad that fakes a button-like behavior with a little rumble effect when pressed uncomfortably hard. Pressing a hard surface really hard did not get comfortable to me, especially not when invoking the multi-tasking switcher with a double-press. And it's awkwardly placed at the very bottom of the device, where my thumb does not rest with any kind of force in normal usage. Another real problem is the lack of third-party browsers<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>, which just leaves you in the cold should a website not work in Safari, or should you dislike Safari's rendering or its very limited ad-blockers. I was particularly annoyed by Safari's context menu for links, which slowly slides out from the link's location, often extending far outside of the screen. Thus opening links in new tabs is just awkwardly slow and annoying in Safari, where Android's more utilitarian menu gets the job done much quicker. Although a phone with a bigger screen might mitigate this problem somewhat.
</p>

<p>
On the topic of animations in general, one of my favorite features in Android is the animation speed setting in the Developer settings. If you want to feel like you got a new, faster phone, just set the animation speed to 0.5x, and marvel at the newfound snappiness of everything.
</p>

<p>
Apps in general feel annoyingly restrictive on iOS. Where is a true Firefox, with addons? Where is a Youtube player that can play a YouTube video in the background while the screen is locked, or block ads (NewPipe is the bee's knees!). Or a file synchronization tool that can actually synchronize useful data outside of its own sandox? In fact, my original hopes for iOS were driven by my memory of the fabled higher quality apps available only on the App Store. Looking at some of my old favorites on the SE however, I was forced to take off those rose-tinted glasses. These apps might have been radical back in the day, but the world has moved on. I don't see a pervasive difference in app quality between iOS and Android any longer. Of course iOS apps do still cost a more money on average, and often can't be test-driven without paying. That two-hour free return policy on the Play Store is seriously genius.
</p>

<p>
Additionally, there were a number of odd problems with the iPhone SE that I found hard to make sense of. For example, apps were very frequently getting booted out of memory. So much so in fact, that often my RSS feeds would not refresh in the background, and podcasts would not download. Even though the iPhone SE sure does have a lot less memory than the Pixel 4a, I hadn't expected this to be an actual problem<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>. And the iPhone would frequently misunderstand a vertical swipe for a horizontal one, or initiate an edge swipe when my finger was still clearly on the screen; My bluetooth headphones sounded noticeably worse, with a strange clipping artifact that should not be there; Significantly worse cell reception and voice quality; Low contrast and tiny text on lock screen notifications; And only barely adequate battery life. And let's not even talk about the stupid Lightning cable, the (comparatively) laggy and tiny TFT screen, the lack of a podcast client I like, and my horrible experiences during the setup process.
</p>

<p>
So, in summary, the iPhone SE was not for me. Don't get me wrong, it's a nice enough phone, and probably enough of a smartphone for most people. But there were a number of issues with both its hardware and its software, where I found Android and the Pixel 4a plainly more productive. Which is surprising, as my mind still had iOS pegged as the premium alternative, that I would totally buy if I wasn't too cheap for it. But in actual use I was annoyed at its preference of form over function, with many a distracting animation and a number of glaring ergonomic mistakes. Well, another lesson learned. The only truly significant advantage of iOS remains its vastly superior software support story: The SE will probably receive software updates until 2025 or 2027, where the best-in-class Pixel 4a will definitely expire in 2023.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">in some places, a right swipe can be used instead of the back button. But the behavior is too inconsistent to be useful</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">what Apple calls "ProRAW", and only makes available in the iPhone 12. iOS does support ordinary RAW, but only in third-party camera apps, and anyway non-stacked RAW files are rather useless from a tiny phone sensor</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">there are different browsers, but they are all required to use Safari's web renderer.</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">I hear this might have been a bug in iOs 14.3? What this says about the software quality of iOS in general is even more troubling, however.</p></div></div>


</div>
</div><div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-computers.html">computers</a> <a href="https://bastibe.de/tag-ui.html">ui</a>
<div class="post-date">27 Oct 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-10-26-differences-between-cameras.html">Differences Between Camera Sensors</a></h1>
<p>
Most cameras have the option to capture <i>raw</i> images, i.e. un-processed image data right from the image sensor. In theory, these images are pure physical measurements of light, and should therefore be very comparable between cameras. But are they? To investigate, I took a <i>raw</i> picture of <a href="https://en.wikipedia.org/wiki/ColorChecker">a color target</a> with each of my five cameras, and compared their output.
</p>

<p>
Capturing accurate colors is a surprisingly intricate matter, as the color depends on the spectrum of the illumination, the reflective spectrum of the colored object, and the color filters in the camera. I normalized the illumination by taking pictures on an overcast day outside<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>, and used a standard color target with 24 colored patches of a certified color.
</p>

<p>
The resulting colored spectra are captured on the camera sensor by photon counters behind three color filters “red”, “green”, and “blue”<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>, which differ from camera to camera in their spectral sensitivity. The recorded colors then get projected on a computer screen with another set of “red”, “green”, and “blue” LEDs of some different spectral makeup, or printed in three or more inks on paper. And this is then seen with eyes of yet another set of “red”, “green”, and “blue” retinal cells. Let's leave it at <i>color is complicated</i>.
</p>

<p>
At any rate, I took pictures at base ISO in <i>raw</i>, white-balanced and exposed for the third grey patch, and adjusted in saturation on the red patch. Here are the colors on the color checker and the recorded colors of my cameras:
</p>

<img style="width:70%;padding-left:15%;padding-top:5px;padding-bottom:10px" src="/static/2020-10/Colorchecker.svg">

<p>
The image shows the 24 colored patches of the color target<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>. Within each patch, five rectangles show the recorded color of (in reading order) the Pentax Q7, the Ricoh GR, the Panasonic LX100, the Fujifilm X-T2, and the Google Pixel 2.
</p>

<p>
Due to my exposure and white balance calibration, the third grey patch is completely equalized and shows no difference between the cameras and the target. The “Light Skin” patch (second on the first row) is also very close to equal, which is an important sanity check for my measurement procedure, as skin tones are optimized by all cameras. Seeing that the skin color patch is one of the most similar hopefully indicates that I did not do anything gravely wrong.
</p>

<p>
The last row shows greys, which are very similar across cameras (excluding the white patch for now). This indicates that RAW values indeed correspond to photon counts, with little additional processing. I have no idea why the white patch is off. Perhaps due to specular reflections on the white patch? Color is complicated.
</p>

<p>
As for the other colors, the Panasonic LX100 (center) and the Fujifilm X-T2 (bottom left) seem to record somewhat more accurate colors than the other three cameras. The Pentax Q7 (top left) and Ricoh GR (top right) seem comparatively weak across the board. The Panasonic (center) is great in darker blues and reds and greens, but somewhat weaker in yellows and pastels. The Fujifilm (bottom left) is very good in almost all colors, with pastels somewhat weaker than darker colors. The Google Pixel 2 (bottom right) is better in reds and greens and yellows than in blues and oranges. Pastel colors in general seem weaker than darker colors, which might be due to the same process that affected the white patch as well.
</p>

<p>
I think I learned something from this experiment: There are indeed color differences between cameras even when shooting RAW, but they are relatively minor. Whenever I see larger differences in my pictures, they are likely (easily correctible) white balance differences, and not sensor differences. When in doubt however, my Panasonic LX100 and Fujifilm X-T2 do capture more lifelike colors than my other cameras.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">any kind of artificial light is comparatively terrible for color reproduction!</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">in quotes, because colors are three-channel information, and we're talking about single channels here.</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">encoded as sRGB, which might or might not be displayed correctly on your screen, depending on your OS, your browser, your settings, your room's illumination, and your screen. Color is complicated.</p></div></div>


</div>
</div><div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-photography.html">photography</a>
<div class="post-date">16 Oct 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-10-16-lenses-matter.html">Lenses Matter</a></h1>
<p>
A common trope in discussions about cameras on the internet is: “sensor size trumps all”, and as a corollary, “smartphone cameras suck”. Which is obviously false to anyone who has ever taken a good picture on a smartphone (with its tiny sensor).
</p>

<p>
The argument, however, goes like this: Bigger pixels<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> can capture a greater dynamic range, and bigger sensors are made from bigger pixels. Therefore, and here is the dangerous leap, cameras with bigger sensors take better pictures.
</p>

<p>
But I currently have a bit of time on my hands, and way too many cameras, so let's try it out, shall we?
</p>

<p>
My five contestants are:
</p>

<img style="width:70%;padding-left:15%;padding-top:5px;padding-bottom:10px" src="/static/2020-10/cameras.jpg">

<ul class="org-ul">
<li>A <a href="https://en.wikipedia.org/wiki/Pixel_2">Google Pixel 2</a>, with a 12.2 MP 5.75 × 4.32 mm “1/2.55 in” sensor from 2017, and a 3.8 mm f/1.8 lens (27 mm f/13 equivalent, crop 7.14),</li>
<li>A <a href="https://en.wikipedia.org/wiki/Pentax_Q7">Pentax Q7</a>, with a 12 MP 7.44 × 5.58 mm “1/1.7 in” sensor from 2013, and a 5-15 mm f/2.8-4.5 lens (24-70 mm f/13-22 equivalent, crop 4.76),</li>
<li>A <a href="https://en.wikipedia.org/wiki/Panasonic_Lumix_DMC-LX100">Panasonic LX100</a>, with a 13 MP, cropped 17.3 × 13 mm “micro four thirds” sensor from 2014, and a 11-34 mm f/1.7-2.8 lens (24-75 mm f/3.8-6 equivalent, crop 2.2),</li>
<li>A <a href="https://en.wikipedia.org/wiki/Ricoh_GR_(large_sensor_compact_camera)">Ricoh GR</a>, with a 16 MP 23.7 × 15.7 mm “APS-C” sensor from 2013, and a 28 mm f/2.8 lens (27 mm f/4.2 equivalent, crop 1.5),</li>
<li>A <a href="https://en.wikipedia.org/wiki/Fujifilm_X-T2">Fujifilm X-T2</a>, with a 24 MP 23.6 × 15.6 mm “APS-C” sensor from 2016, and a 18-135 mm f/3.5-5.6 lens (27-200 mm f/5.25-8.4 equivalent, crop 1.5).</li>
</ul>

<p>
So I went for a walk around the neighborhood, and took a few pictures with a bag full of cameras. In particular, I was looking for a high dynamic range scene, in the form of a sunset. All pictures were taken at a low ISO<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup> and underexposed by two stops to account for the backlighting. All images were taken at 27 mm (equivalent<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>) focal length.
</p>

<p>
<a href="https://photonstophotos.net/Charts/PDR.htm#FujiFilm%20X-T2,Google%20Pixel%202,Panasonic%20Lumix%20DMC-LX100,Pentax%20Q7,Ricoh%20GR">Measurements predict</a> that the dynamic range and noise levels of these cameras should get better as the sensor size increases.
</p>

<br>
<a href="/static/2020-10/scene-1-lightbox.html" target="_blank">
  <div class="lightbox" style="height: 200px">
    <figure>
      <img src="/static/2020-10/Scene 1 Google Pixel 2.thumb.jpg">
      <figcaption>Google Pixel 2</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 1 Pentax Q7.thumb.jpg">
      <figcaption>Pentax Q7</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 1 Panasonic DMC-LX100.thumb.jpg">
      <figcaption>Panasonic LX100</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 1 Ricoh GR.thumb.jpg">
      <figcaption>Ricoh GR</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 1 Fujifilm X-T2.thumb.jpg">
      <figcaption>Fujifilm X-T2</figcaption>
    </figure>
  </div>
</a>

<p>
The first scene was taken essentially in auto mode, and adjusted in Darktable to match roughly in tonality (exposure adjustments of ±0.25EV), but not color. In general, all cameras were able to capture the entire dynamic range of this scene, from the brightest almost-white area around the sun, to the darkest patches in the shade near the waterline.
</p>

<p>
This, right off the bat, is the most important result of this experiment: <i>Most scenes do not have enough dynamic range to matter</i>.
</p>

<p>
Differences appear in the noise levels, which are fairly constant across all cameras, with only the X-T2 being noticeably less noisy than the rest. However, this advantage of the X-T2 is not one of sensor size (the Ricoh GR has a same-size sensor), but simply of being two-generations <i>newer</i>.
</p>

<p>
The largest difference between these photos is instead in flare: The Pixel 2, Pentax Q7 and Ricoh GR are flaring across the entire image, while the LX100 and X-T2 are far more controlled. This is probably because the Pixel 2 and Ricoh GR have collected dust and scratches from my pockets, and the Pentax Q7 lens being somewhat mediocre. The LX100 and X-T2 are in a better condition, and a better design, too. All lenses also show some amount of colorful lens flares to the bottom-right of the sun, particularly in the LX100 and Pentax Q7, and strange, colored rings around the edge of the Ricoh GR's frame.
</p>

<br>
<a href="/static/2020-10/scene-2-lightbox.html" target="_blank">
  <div class="lightbox" style="height: 200px">
    <figure>
      <img src="/static/2020-10/Scene 2 Google Pixel 2.thumb.jpg">
      <figcaption>Google Pixel 2</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Pentax Q7.thumb.jpg">
      <figcaption>Pentax Q7</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Panasonic DMC-LX100.thumb.jpg">
      <figcaption>Panasonic LX100</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Ricoh GR.thumb.jpg">
      <figcaption>Ricoh GR</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Fujifilm X-T2.thumb.jpg">
      <figcaption>Fujifilm X-T2</figcaption>
    </figure>
  </div>
</a>

<p>
To look into these differences between lenses in more detail, the second scene was taken from a slightly different spot, with the lenses stopped down (if possible). This image includes two bright sun spots, one from the sun itself, and one in the reflection on the water.
</p>

<p>
Notably, the Pixel 2, like most smartphones, can not change its aperture. And it produces a bright red flare towards the left edge of the image. The Pentax Q7 again flares dramatically and colorful, and my Ricoh GR's lens is probably beyond cleaning at this point, judging from the massive flare it produces. Again, the standout pictures here are the X-T2 and LX100, with well-defined sun stars and comparatively little flaring. (Note the red dots in the X-T2: these are the phase-detection pixels on the X-T2's sensor reflecting light back into the lens and back on the image.)
</p>

<br>
<a href="/static/2020-10/scene-2-crop-lightbox.html" target="_blank">
  <div class="lightbox" style="height: 200px">
    <figure>
      <img src="/static/2020-10/Scene 2 Crop Google Pixel 2.thumb.jpg">
      <figcaption>Google Pixel 2</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Crop Pentax Q7.thumb.jpg">
      <figcaption>Pentax Q7</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Crop Panasonic DMC-LX100.thumb.jpg">
      <figcaption>Panasonic LX100</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Crop Ricoh GR.thumb.jpg">
      <figcaption>Ricoh GR</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 2 Crop Fujifilm X-T2.thumb.jpg">
      <figcaption>Fujifilm X-T2</figcaption>
    </figure>
  </div>
</a>

<p>
Here is an enlargement of the dark patch at the right edge of the second scene, to judge noise levels and detail in the shadow. Perhaps surprisingly, the differences between these images are not big at all. The X-T2 clearly wins in terms of both shadow detail and noise levels, but by a rather small margin, and only really visible when enlarged. The Pentax Q7 resolves almost no detail in the shadow, but neither does the Ricoh GR. So sensor size does not seem to affect this round.
</p>

<br>
<a href="/static/2020-10/scene-3-lightbox.html" target="_blank">
  <div class="lightbox" style="height: 200px">
    <figure>
      <img src="/static/2020-10/Scene 3 Google Pixel 2.thumb.jpg">
      <figcaption>Google Pixel 2</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Pentax Q7.thumb.jpg">
      <figcaption>Pentax Q7</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Panasonic DMC-LX100.thumb.jpg">
      <figcaption>Panasonic LX100</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Ricoh GR.thumb.jpg">
      <figcaption>Ricoh GR</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Fujifilm X-T2.thumb.jpg">
      <figcaption>Fujifilm X-T2</figcaption>
    </figure>
  </div>
</a>

<p>
This third scene was shot one day later, with settings optimized for optimal image quality: Base ISO, aperture one stop closed from wide open, focused on a high-contrast part of the scene. I processed these images with white balance from a grey card, then edited exposure, contrast, and saturation to match across images. Chromatic aberrations and vignetting were corrected using a profile or by hand. No sharpening was applied, but the highest-quality deinterlacing was selected (AMaZe for Bayer sensors, Markensteijn 3-pass for X-Trans sensors).
</p>

<p>
At this magnification, even my 24in/4K screen only barely hints at differences in sharpness. Which implies that prints up to A3 should be possible from all of these cameras without much difference in perceived sharpness.
</p>

<br>
<a href="/static/2020-10/scene-3-crop-lightbox.html" target="_blank">
  <div class="lightbox" style="height: 200px">
    <figure>
      <img src="/static/2020-10/Scene 3 Crop Google Pixel 2.thumb.jpg">
      <figcaption>Google Pixel 2</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Crop Pentax Q7.thumb.jpg">
      <figcaption>Pentax Q7</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Crop Panasonic DMC-LX100.thumb.jpg">
      <figcaption>Panasonic LX100</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Crop Ricoh GR.thumb.jpg">
      <figcaption>Ricoh GR</figcaption>
    </figure>
    <figure>
      <img src="/static/2020-10/Scene 3 Crop Fujifilm X-T2.thumb.jpg">
      <figcaption>Fujifilm X-T2</figcaption>
    </figure>
  </div>
</a>

<p>
If we crop into these images, differences in detail resolution indeed become apparent. The Pentax Q7 is definitely the least detailed, on account of its soft lens. The Pixel 2 and LX100 show a very similar amount of details, which is a testament to how good modern smart phone cameras truly have become. While the two cameras with the biggest sensors, the Ricoh GR and the X-T2, do resolve visibly more detail, the differences are not dramatic.
</p>

<p>
So does sensor size trump all? From these experiments, I wouldn't say so. Essentially, all of these cameras took decent images. The X-T2 clearly was better than the rest, but not hugely so, while being <i>much</i> bigger and more expensive and newer.
</p>

<p>
While the measured dynamic range differences surely exist, they did not manifest even in a sunrise, so I'm inclined to discount their importance. This is, frankly, not what I expected. The miniscule differences in resolution were equally unexpected.
</p>

<p>
What did however make a big difference was lens quality. Flare control, aperture range, and focal range play a huge role in the resulting image quality. Zooming in (and stopping down) are still the most effective way of extracting more detail from a scene. And having a camera with a secure grip that is quick to operate helps, too. I think I am growing out of sensor size snobbery, is what I'm saying.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">technically, “sensor elements”, not “picture elements”, i.e. “sensels”</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">lower ISO → less noise.</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">the focal length a lens on a 35 mm film camera would need to cover the same angle of view.</p></div></div>


</div>
</div><div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-photography.html">photography</a>
<div class="post-date">17 Sep 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-09-17-dear-computer-we-need-to-talk.html">Dear Computer, We Need to Talk</a></h1>
<p>
After years of using Linux on my desktop, I decided to install Windows on my computer, to get access to a few commercial photo editing applications. I'll go into my grievances with Linux later, but for now:
</p>

<div id="outline-container-org4bd5f22" class="outline-2">
<h2 id="org4bd5f22">I tried to install Windows, you won't believe what happened next</h2>
<div class="outline-text-2" id="text-org4bd5f22">
<p>
Like I have done many times with Linux, I download a Windows image from my university, and write it to a USB drive, then reboot into the USB drive. The USB drive can't be booted. A quick internet search leads me to a Microsoft Support page on how to <a href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/install-windows-from-a-usb-flash-drive">Install Windows from a USB Flash Drive</a>, which says that
</p>

<img style="width:50%;padding-left:25%" src="/static/2020-09/one does not simply.jpg">

<p>
Instead, one has to format the stick as FAT32, make it <i>active</i>, then copy the files from the image to it. So I follow the instructions and open the Disk Management program. It does not offer an option of FAT32, nor for making the partition <i>active</i>. I settle on (inactive) exFAT instead. It doesn't boot.
</p>

<p>
I switch over to Linux, where I can indeed make a FAT32 partition, and I can mark it as <i>bootable</i>, which I take as the equivalent of <i>active</i>. But Linux can not open the Windows image to copy the files onto the USB stick. So back to Windows, for copying the files. Except they can't be copied, because some of them are larger than 4Gb, which can't be written to a FAT32 partition. What now?
</p>

<p>
While researching how to download a different version of Windows 10, I stumble upon the <a href="https://www.microsoft.com/de-de/software-download/windows10">Media Creation Tool</a>, which automatically downloads Windows 10 and writes it to the USB stick correctly. Why was this not pointed out in the article above? Who knows. At any rate, it works. I can finally install Windows.
</p>

<p>
The installation process requires the usual dozen-or-so refusals of tracking, ads, privacy intrusions, and voice assistants. I wish I could simply reject them all at once. And then the install hangs, while "polishing up a few things". Pressing the helpful back button, and then immediately the forward button unhangs it, and the installation completes.
</p>

<p>
Next up are drivers. It feels anachronistic to have to install drivers manually in this day and age, but oh well. The new GPU driver to make screen tearing go away, a driver for my trackball to recognize the third mouse button, a wacom driver, ten or so Intel drivers of unknown utility. The trackball driver is not signed. I install it anyway. The GPU driver does not recognize my GPU and can't be installed. A quick Internet search reveals that my particular AMD/Intel GPU/CPU was discontinued from support by both AMD and Intel, and does not have a current driver. But fora suggest that up to version 20.2.1 of the AMD driver work fine. They don't, the driver crashes when I open images in my photo editor. An even older version published by Intel does work correctly. So now I am running an AMD GPU with an Intel driver from 2018.
</p>

<img style="width:50%;padding-left:25%" src="/static/2020-09/this is fine.png">

<p>
Installing and setting up Firefox and my photo editors works without issue, thank goodness. Emacs has a Windows installer now, which is greatly appreciated. OpenCL and network shares just work. This is why I'm installing Windows next to my Linux.
</p>

<p>
But Windows is still not activated. I copy my university's product key in the appropriate text box, but hesitate: That's for Windows Enterprise, and I'd be just fine with Home. So I cancel the activation without activating. A helpful link in the activation systems sends me to the Microsoft Store to get my very own version of Windows Home for €145, which normally retails for around €95, so that's a no-go. Whatever, I'll go with my university's Enterprise edition. Except the activation box now says my product key is invalid. And the Store now literally says "We don't know how you got here but you shouldn't be here" instead of selling me Windows. After a restart it installs and activates Windows Enterprise, even though I never actually completed the activation.
</p>

<img style="width:70%;padding-left:15%" src="/static/2020-09/this is fine 2.jpg">

<p>
I install Git, but in order to access my Github I need to copy over my SSH key from the Linux install. Which I can't boot at the moment, because installing Windows overwrites the boot loader. This is normal. So I download Ubuntu, write it to the USB stick, boot into it, recover the bootloader, boot into my old install, reformat the stick, copy the files to the stick, boot back into Windows, and the files aren't on the stick. Tough. Boot back into Linux, copy the files onto the stick, <i>eject the stick</i>, boot back into Windows, copy the files to the computer. Great user experience.
</p>

<p>
Now that I have my SSH key, I open a Git Bash to download a project. It says my credentials are incorrect. I execute the same commands in a regular CMD instead of Git Bash, and now my credentials are correct. Obviously.
</p>

<p>
There are several programs that claim to be able to read Linux file systems from Windows. They do not work. But Microsoft has just announced that you will be able to mount Linux file systems from WSL in a few weeks or months. So maybe that will work!
</p>

<p>
I set my lock screen to a slideshow of my pictures. Except my pictures do not show up, and I get to see Window's default pictures instead. An internet search reveals that this is a wide-spread problem. Many "solutions" are offered <a href="https://answers.microsoft.com/en-us/windows/forum/windows_10-start/windows-10-lock-screen-slideshow-not-showing/01975f7f-11e8-457e-a8ef-5b494af135f1">in the support fora</a>. What works for me is to first set the lock screen to "Windows Spotlight", then to "Slideshow". Only in that order will my pictures be shown.
</p>

<p>
I will stop here. I could probably go on ad infinitum if I wanted to. This was my experience of using Windows for one day. I consider these problems relatively benign, in that all of them had solutions, if non-obvious ones.
</p>
</div>
</div>

<div id="outline-container-org5e7ec5e" class="outline-2">
<h2 id="org5e7ec5e">Why install Windows in the first place?</h2>
<div class="outline-text-2" id="text-org5e7ec5e">
<p>
Part of the reason for installing Windows was my growing frustration with Linux. I have been a happy user of KDE of various flavors for about seven years now. But ever since I got into photo editing, things began to become problematic:
</p>

<p>
My photo editor requires OpenCL, but the graphics driver situation on Linux is problematic, to say the least. I generally managed to get RocM running most of the time, but kernel updates frequently broke it, or required down- or upgrading RocM. It was a constant struggle.
</p>

<p>
I wanted to work with some of my data on a network share, but KDE's implementation of network shares does not simply mount them for applications to use, but instead requires each application to be able to open network locations on their own. Needless to say, this worked almost never, requiring many unnecessary file copies. Perhaps Gnome handles network shares better, but let's not open that can of worms.
</p>

<p>
Printing photos simply never worked right for me. The colors were off, photo papers were not supported, the networked printer was rarely recognized. Both for a Samsung printer and an Epson and a Canon. One time a <a href="https://turboprint.info/">commercial printer driver for Linux</a> printed with so much ink it dripped off the paper afterwards. Neither Darktable nor Gimp nor Digikam have a robust printing mode. I generally resorted to Windows for printing.
</p>

<p>
I ran that Windows in a virtual machine. With Virtualbox, the virtual machine would be extremely slow, to the point where it had a delay of several seconds between typing and seeing letters on the screen. VMWare did better, but would suddenly freeze and hang for minutes at a time. Disabling hugepages helped sometimes, for a short while. The virtual machine network was extremely unreliable. Some of these issues were probably related to my using a 4K screen.
</p>

<p>
Speaking of screens, I have two screens, one HighDPI 4k and one normal 1440p. Using X, the system can be either in HighDPI mode, or in normal mode. But it can't drive the two displays in different modes. Thus the second monitor was almost useless and I generally worked only on the 4k screen. With Wayland I would have been able to use both screens in different modes, but not be able to color-calibrate them or record screen casts. Which is completely unacceptable. So I stuck with one screen and X. In Windows, I can use both screens and calibrate them.
</p>

<img style="width:50%;padding-left:25%" src="/static/2020-09/wtf.jpg">


<p>
Additionally, Linux hardware support is still a bit spotty. My SD card reader couldn't read some SD cards because of driver issues. It would sometimes corrupt the SD card's file systems. USB-connected cameras were generally not accessible. The web cam did not work reliably. The CPU fan ran too hot most of the time.
</p>

<p>
So there had been numerous grievances in Linux that had no solutions. Still I stuck with it because so many more smaller issues were actually fixable if I put in the work. In fact I had accumulated quite a number of small hacks and scripts for various issues. I feared that Windows would leave me without recourse in these situations. And it doesn't. But at least the bigger features generally work as advertised.
</p>
</div>
</div>

<div id="outline-container-org31bc442" class="outline-2">
<h2 id="org31bc442">Where do we go from here?</h2>
<div class="outline-text-2" id="text-org31bc442">
<p>
Just for completion's sake, I should really find an Apple computer and run it through its paces. From my experience of occasionally using a Macbook for teaching over the last few years, I am confident that it fares no better than Linux or Windows.
</p>

<p>
Were things always this broken? How are normal people expected to deal with these things? No wonder every sane person now prefers a smartphone or tablet to their computers. Limited as they may be, at least they generally <i>work</i>.
</p>

<p>
There is no joy in technology any more.
</p>
</div>
</div>
<div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-computers.html">computers</a> <a href="https://bastibe.de/tag-linux.html">linux</a> <a href="https://bastibe.de/tag-windows.html">windows</a>
<div class="post-date">27 May 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-05-27-how-to-write-a-dissertation.html">How to Write a Dissertation</a></h1>
<p>
Assembling scientific documents is a complex task. My documents are a combination of graphs, data, and text, written in LaTeX. This post is about combining these elements, keeping them up to date, while not losing your mind. My techniques work on any Unix system on Linux, macOS, or the WSL.
</p>

<div id="outline-container-org18d2b13" class="outline-2">
<h2 id="org18d2b13">Text</h2>
<div class="outline-text-2" id="text-org18d2b13">
<p>
For engineering or science work, my deliverables are PDFs, typically rendered from LaTeX. But writing LaTeX is not the most pleasant of writing environments. So I've tried my hand at org-mode and Markdown, compiled them to LaTeX, and then to PDF. In general, this worked well, but there always came a point where the abstraction broke, and the LaTeX leaked up the stack into my document. At which point I'd essentially write LaTeX anyway, just with a different syntax. After a few years of this, I decided to cut the middle-man, bite the bullet, and just write LaTeX.
</p>

<p>
That said, modern LaTeX is not so bad any more: XeLaTeX supports normal OpenType fonts, mixed languages, proper unicode, and natively renders to PDF. It also renders pretty quickly. My entire dissertation renders in less than three seconds, which is plenty fast enough for me.
</p>

<p>
To render, I run a simple makefile in an infinite loop that recompiles my PDF whenever the TeX source changes, giving live feedback while writing:
</p>

<div class="org-src-container">
<pre class="src src-makefile"><span style="color: #111111; text-decoration: underline;">diss.pdf</span>: diss.tex makefile $(<span style="color: #404040;">graph_pdfs</span>)
    xelatex -interaction nonstopmode diss.tex
</pre>
</div>

<p>
We'll get back to <code>$(graph_pdfs)</code> in a second.
</p>
</div>
</div>

<div id="outline-container-org318c507" class="outline-2">
<h2 id="org318c507">Graphs</h2>
<div class="outline-text-2" id="text-org318c507">
<p>
A major challenge in writing a technical document is keeping all the source data in sync with the document. To make sure that all graphs are up to date, I plug them into the same makefile as above, but with a twist: All my graphs are created from Python scripts of the same name in the <code>graphs</code> directory.
</p>

<p>
But you don't want to simply execute <i>all</i> the scripts in <code>graphs</code>, as some of them might be shared dependencies that do not produce PDFs. So instead, I only execute scripts that start with a chapter number, which conveniently sorts them by chapter in the file manager, as well.
</p>

<p>
Thus all graphs render into the main PDF and update automatically, just like the main document:
</p>

<div class="org-src-container">
<pre class="src src-makefile"><span style="color: #404040;">graph_sources</span> = $(<span style="color: #404040;">shell</span> find graphs -regex <span style="color: #606060;">"graphs/[0-9]-.*\.py"</span>)
<span style="color: #404040;">graph_pdfs</span> = $(<span style="color: #404040;">patsubst</span> %.py,%.pdf,$(<span style="color: #404040;">graph_sources</span>))

<span style="color: #111111; text-decoration: underline;">graphs/%.pdf</span>: graphs/%.py
    cd graphs; .venv/bin/python $(<span style="color: #404040;">notdir</span> $<span style="color: #111111;">&lt;</span>)
</pre>
</div>

<p>
The first two lines build a list of all graph scripts in the <code>graphs</code> directory, and their matching PDFs. The last two lines are a makefile recipy that compiles any graph script into a PDF, using the virtualenv in <code>graphs/.venv/</code>. How elegant these makefiles are, with recipe definitions independent of targets.
</p>

<p>
This system is surprisingly flexible, and absolutely trivial to debug. For example, I sometimes use those graph scripts as glorified shell scripts, for converting an SVG to PDF with Inkscape or some similar task. Or I compile some intermediate data before actually building the graph, and cache them for later use. Just make sure to set an appropriate exit code in the graph script, to signal to the makefile whether the graph was successfully created. An additional makefile target <code>graphs: $(graph_pdfs)</code> can also come in handy if you want ignore the TeX side of things for a bit.
</p>
</div>
</div>

<div id="outline-container-org69a93d6" class="outline-2">
<h2 id="org69a93d6">Data</h2>
<div class="outline-text-2" id="text-org69a93d6">
<p>
All of the graph scripts and TeX are of course backed by a Git repository. But my dissertation also contains a number of databases that are far too big for Git. Instead, I rely on git-annex to synchronize data across machines from a simple webdav host.
</p>

<p>
To set up a new writing environment from scratch, all I need is the following series of commands:
</p>

<div class="org-src-container">
<pre class="src src-shell">git clone git://mygitserver/dissertation.git dissertation
<span style="color: #404040;">cd</span> dissertation
git annex init
env <span style="color: #404040;">WEBDAV_USERNAME</span>=xxx <span style="color: #404040;">WEBDAV_PASSWORD</span>=yyy git annex enableremote mywebdavserver
git annex copy --from mywebdavserver
(<span style="color: #404040;">cd</span> graphs; pipenv install)
make all
</pre>
</div>

<p>
This will download my graphs and text from <code>mygitserver</code>, download my databases from <code>mywebdavserver</code>, build my Python environment with <code>pipenv</code>, recreate all the graph PDFs, and compile the TeX. A process that can take a few hours, but is completely automated and reliable.
</p>

<p>
And that is truly the key part; The last thing you want to do while writing is being distracted by technical issues such as "where did I put that database again?", "didn't that graph show something different the other day?", or "I forgot to my database file at work and now I'm stuck at home during the pandemic and can't progress". Not that any of those would have ever happened to me, of course.
</p>
</div>
</div>
<div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-computers.html">computers</a> <a href="https://bastibe.de/tag-emacs.html">emacs</a> <a href="https://bastibe.de/tag-workflow.html">workflow</a> <div id="archive">
<a href="https://bastibe.de/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status"><center><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br /><span xmlns:dct="https://purl.org/dc/terms/" href="https://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">bastibe.de</span> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://bastibe.de" property="cc:attributionName" rel="cc:attributionURL">Bastian Bechtold</a> is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</center></div>
</body>
</html>
