<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://bastibe.de/rss.xml"
      title="RSS feed for https://bastibe.de/"/>
<title>Bastibe.de</title>
<meta name="author" content="Bastian Bechtold">
<meta name="referrer" content="no-referrer">
<link href= "static/style.css" rel="stylesheet" type="text/css" />
<link rel="icon" href="static/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="static/favicon-152.png">
<link rel="msapplication-TitleImage" href="static/favicon-144.png">
<link rel="msapplication-TitleColor" href="#0141ff">
<script src="static/katex.min.js"></script>
<script src="static/auto-render.min.js"></script>
<link rel="stylesheet" href="static/katex.min.css">
<script>document.addEventListener("DOMContentLoaded", function() { renderMathInElement(document.body); });</script>
<meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8">
<meta name="viewport" content="initial-scale=1,width=device-width,minimum-scale=1"></head>
<body>
<div id="preamble" class="status"><div class="header">
  <a href="https://bastibe.de">Basti's Scratchpad on the Internet</a>
  <div class="sitelinks">
    <a href="https://twitter.com/paperflyer">Twitter</a> | <a href="https://github.com/bastibe">Github</a> | <a href="https://bastibe.de/projects.html">Projects</a>
  </div>
</div></div>
<div id="content">

<div class="post-date">27 May 2021</div><h1 class="post-title"><a href="https://bastibe.de/2021-05-27-camera-resolution.html">Camera Resolution</a></h1>
<p>
Some of my photographic lenses are reknown for their outstanding sharpness, others are said to be mediocre. But somehow I never quite saw a big difference in sharpness between them. Do older lenses have less resolution than the newer ones? Can a dedicated prime lens resolve more detail than a multi-purpose zoom lens? Let's find out.
</p>

<p>
These questions recently came to a head for me, in the choice between two compact cameras: <a href="https://camerasize.com/compact/#566,819,ha,f">A Fujifilm X100T, 16 MP, with a 23 mm lens, and a Ricoh GR III, 24 MP, with an 18 mm lens</a>. In the last few months, I grew to like the X100T's 23 mm viewpoint, but did not enjoy how bulky the camera is in comparison to the GR III. So I wondered if I could replace the X100T with the Ricoh GR III and simply crop to 23 mm.
</p>

<p>
Being a scientist and all that, I set up an experiment: I printed out a <a href="https://www.graphics.cornell.edu/~westin/misc/res-chart.html">resolution chart</a>, set my cameras on a tripod, and took pictures with all my cameras such that the chart filled a similar portion of each image. Base ISO, two-second timer, processed in Capture One. White balance and contrast were equalized.
</p>

<figure style="text-align: center;">
<a href="/static/2021-05/comparison.svg"><img src="/static/2021-05/comparison.svg" alt="camera resolution comparison" width="80%"/></a>
<figcaption>Red bars indicate the point where some lines are no longer distinguishable, i.e. the limit of the system's resolution.</figcaption>
</figure>

<p>
Each row in the above grid is one camera/lens/focal length combination, taken at the brightest aperture in the left column, and at the sharpest aperture in the right column. The text on the left shows photographic parameters, and the text on the right image parameters. The red line is the spot on the chart where some lines start to blend together, the limit of the image's resolution, as judged by my eyeballs. Depending on the exact framing, DPIs vary somewhat between the images, but generally at a factor of around √(̅2̅4̅/̅1̅6̅)̅ = 1.22 between the 24 MP images and the 16 MP images. The bottom row shows a synthetic image at simulated 24 MP / 380 DPI and 16 MP / 280 DPI.
</p>

<p>
From these images, the biggest contribution to resolution seems to be sensor megapixels. All 24 MP images resolve closer line pairs than any 16 MP image. This could also be a hint that lenses seem to generally outresolve their sensors.
</p>

<p>
Looking closely at the images, another hint becomes visible: aliasing is visible in all images. This is only possible if lens resolution is higher than sensor resolution.
</p>

<p>
Frankly, this is a rather big surprise to me. Reading discussions on the internet had predisposed me to believe that some lenses should be much higher resolution than others, and my big travel-zoom in particular should be terribly soft. From my measurements however, I see no evidence of this claim. All of these lenses outresolve their sensors, and differences between lenses is rather miniscule in general.
</p>

<p>
Another common wisdom is that lenses wide open are less sharp than stopped down. In this test sample, this seems to be true for the X100T's, and to a lesser extent for the LX100, and the Fujifilm XF 60. All other lenses do not show significant (!) differences between apertures. And even for the affected lenses, the difference in resolution is not dramatic.
</p>

<p>
However, resolution is not sharpness. While line pairs may remain resolvable, their edges do lose definition at wide-open apertures on the Fujifilm XF 18-135 at some focal lengths, and the X100T. These lenses definitely need at least additional sharpening at wider apertures. Contrast is also visibly different, but I chose to edit out contrast differences to keep the graphs legible. Generally, the Ricoh GRs' lenses (particularly the GR I) were by far the most contrasty of these lenses.
</p>

<p>
Another interesting comparison can be drawn between the synthetic images in the bottom row and the camera exposures above: None of the 24 MP images reach the resolution of the synthetic image, perhaps due to demosaicing artifacts. However, the 16 MP exposures are very close to the synthetic image. I don't yet know how to interpret this.
</p>

<p>
Speaking of demosaicing however, there does not seem to be a difference in resolution between the X-Trans Fujifilm sensors and the Bayer Ricoh/Panasonic sensors. I also checked whether a different raw developer would make a difference, but did not find anything noteworthy.
</p>

<p>
Overall, this experiment has taught me that all my lenses are high resolution, and that lens resolution in general is probably not a topic worth fussing over. Subpixel patterns are equally unimportant. Instead, lenses do differentiate themselves in contrast and sharpness, and camera sensors in megapixels. And finally, the tiny GR III in crop mode can indeed outresolve the X100T, and out-contrast and out-sharpness it as well.
</p>
<div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-photography.html">photography</a>
<div class="post-date">05 May 2021</div><h1 class="post-title"><a href="https://bastibe.de/2021-05-05-books-of-2020.html">Books of 2020</a></h1>

<div id="outline-container-org519b7ea" class="outline-2">
<h2 id="org519b7ea">Chickenhawk</h2>
<div class="outline-text-2" id="text-org519b7ea">
<figure style="float:left">
<img src="/static/2021-05/chickenhawk cover.jpg" alt="chickenhawk cover" width="150px"/>
</figure>

<p>
Most first hand accounts of wars I have read are written by a somewhat amateur author. Not this one. Robert Mason infused his terrifying tale with plenty of drama and humanity, and managed to write one of the best personal military history I have ever read.
</p>

<p>
Part of it is surely the subject matter, as I am a huge fan of aviation and its lore. And part of it is that I haven't yet read a lot about the Vietnam War.
</p>

<p>
But either way, I could not put this book down. Every mission seems to be more dramatic than the last, and you can see Mason's flying and survival skills just barely keeping up with the challenge. You can also viscerally feel the surviver bias at work, with plenty of close calls and dead friends. It's terrifying and thrilling at the same time.
</p>

<p>
And towards the end, the book truly surprised me with a very frank account of PTSD and the life of a veteran, which I hadn't read anywhere else in this level of clarity.
</p>

<p>
I cannot recommend this book enough. Perhaps a bit depressing at times, but an utterly compelling story, and without a doubt one of the best books I have ever read.
</p>
</div>
</div>

<div id="outline-container-orgd30c9c8" class="outline-2">
<h2 id="orgd30c9c8">Mountain Light</h2>
<div class="outline-text-2" id="text-orgd30c9c8">
<figure style="float:left">
<img src="/static/2021-05/mountain light cover.jpg" alt="mountain light cover" width="150px"/>
</figure>

<p>
Quite simply, the best photography book I have ever read. From the era of color film, that short time between the black-and-white darkroom and photoshop, where pictures really couldn't be edited. It all came down to the skill of the photographer, which makes a book from this era much more useful than its darkroom-edited predecessors or photoshopped successors.
</p>

<p>
Besides that, the story of an avid adventurer who took part in several expeditions to climb the most difficult mountains in the world for the first time, and document life in the most remote places on earth before man could ever touch them.
</p>

<p>
All in all, a fantastic book of outdoorsmanship, adventures, and photography, three things I love dearly.
</p>
</div>
</div>

<div id="outline-container-orgb38d374" class="outline-2">
<h2 id="orgb38d374">Two Years Before the Mast</h2>
<div class="outline-text-2" id="text-orgb38d374">
<figure style="float:left">
<img src="/static/2021-05/two years before the mast cover.jpg" alt="two years before the mast cover" width="150px"/>
</figure>

<p>
The story of the young author joining a trader ship to the US west coast, in 1834, when it was still largely uninhabited and wilderness. They traveled to many a well-known place, such as San Francisco or Santa Barbara, but these places were still merely trading outposts and missions.
</p>

<p>
At the same time, the author chronicles a sailor's life on board the last generation of sailing vessels, just before they were replaced by steamers. He joined the ship on a two-year-or-so contract around the American continent, on an entirely self-sufficient ship with only the occasional sighting of a fellow ship or the lighting of cargo in a trading depot. Which is an utterly fascinating historical perspective considering that the story happens more-or-less at the same time as the comparatively modern-seeming Sherlock Holmes.
</p>

<p>
But a true eye-opener happens at the very end, where the author re-visits San Francisco later in his life. The city by then has grown to a bustling trade town with regular ferry routes and rail connections, churches and pubs, and all the amenities of modern life. It is hard to believe how such industry would grow from these humble beginnings in just a few decades.
</p>
</div>
</div>

<div id="outline-container-org2f12f51" class="outline-2">
<h2 id="org2f12f51">Bobiverse: We Are Legion / For We Are Many / All These Worlds</h2>
<div class="outline-text-2" id="text-org2f12f51">
<figure style="float:left">
<img src="/static/2021-05/we are legion cover.jpg" alt="we are legion cover" width="150px"/>
</figure>

<p>
Like good candy, I just couldn't stop reading. The story of a regular guy, uploaded into a colony ship as an AI, roaming the galaxy, and exploring the boundaries of what it means to be human. Which sounds much more philosophical than it actually is. The book is written in a lighthearted and fast-paced style that I could hardly put down. I read the first book in one week, and immediately followed through with book two and three.
</p>

<p>
And it actually comes to a satisfying end in the third book, instead of methastesizing into a franchise, which I can't praise highly enough. Perhaps a little bit superficial and handwavy at times, but a thoroughly good time for a science fiction fan.
</p>
</div>
</div>
<div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-books.html">books</a>
<div class="post-date">04 May 2021</div><h1 class="post-title"><a href="https://bastibe.de/2021-05-04-a-review-of-the-raw-photo-editor-capture-one.html">A Review of the RAW Photo Editor Capture One</a></h1>
<p>
About half a year ago, my second child was born, and all that precious free time I had used for photo editing evaporated. So I started looking for faster photo editor, as a temporary replacement for <a href="https://www.darktable.org/">Darktable</a>. Based on my <a href="https://bastibe.de/2020-05-01-raw-developer-comparison.html">research last time</a>, I chose <a href="https://www.captureone.com/en">Capture One</a>. During the last six months, I got to know the program well, and want to share my thoughts on it, and how it compares to Darktable.
</p>

<p>
Let's start with a bit of background about my photography, to put my views into context. I learned photo editing on Linux, with <a href="https://www.darktable.org/">Darktable</a>, mostly for documenting my travels and family. I gather that a photographer who does not know Lightroom is a bit of an oddity. But it should provide an interesting perspective for the purposes of this review. Furthermore, my interest in post processing is not purely artistic. Being a signal processing scientist, I can't help but analyze the underlying image processing algorithms, and am perhaps more annoyed by certain editing artifacts than most people.
</p>

<p>
In terms of photography, I tend towards quick reactionary shooting of the fleeting moments of my family's life. Mostly because I prefer to spend my time playing with my kids instead of pointing a camera at them. I am therefore perhaps a bit more tolerant of slight imperfections in terms of focus and exposure than is usual. This also means I value my camera/editor's capability for recovering misjudged exposures or imperfect lighting very highly.
</p>

<p>
Most review articles of photo editing software are conducted from only a short glance at the contesting programs, since spending the months necessary to get to know each program is usually neither practical nor affordable. But due to my particular circumstances, the present text actually comes from a few months of more-or-less exclusive use of Capture One, on about 30 thousand pictures, of which I edited about 2000 (all new, not just re-edits of old ones, and implicitly the incentive to recreate another program’s rendering).
</p>

<figure style="width: 100%; text-align: center;">
<a href="/static/2021-05/develop screen.png" target="_blank"><img width="75%" src="/static/2021-05/develop screen.png" alt="Capture One screen layout" /></a>
<figcaption>Capture One's main develop screen </figcaption>
</figure>

<p>
So, Capture One. It is very fast. You move a slider, you see the results of that change immediately, even on my lowly Surface 7 Pro tablet. But this focus on speed actually goes deeper than just slider movements: I discovered that many parts of the application seemed actively optimized for quick operation, which allows me to process pictures much more quickly. This was frankly a revelation in the time-starved months after my second baby’s birth.
</p>

<p>
As an example of how much quicker Capture One's default workflow is than Darktable's, here's my usual editing flow in Darktable: I start with fixing exposure, in the Exposure module. Then I adjust shadow density and recover highlight color with the black and white sliders in the Filmic RGB module. Then I adjust white balance to taste in the Color Calibration module. Then I crop in Crop and Rotate, then add additional adjustments such as Color Zones, Color Balance, Tone Equalizer, Denoise (Profiled), or Contrast Equalizer. Last come local adjustments if necessary. The thing is, more or less every image needs at least Exposure adjustments, some Filmic RGB tweaking, and cropping. All of these items are situated in different modules in Darktable, each requiring multiple clicks to access the module and then change the value. This is supposed to be improved in the next version of Darktable, which will let you put all your favorite sliders in a custom module. I am very much looking forward to that.
</p>

<figure style="width: 100%; text-align: center;">
<img width="24%" src="/static/2021-05/color calibration.png" alt="Darktable color calibration" />
<img width="24%" src="/static/2021-05/crop and rotate.png" alt="Darktable crop and rotate" />
<img width="24%" src="/static/2021-05/filmic rgb.png" alt="Darktable filmic RGB" />
<img width="24%" src="/static/2021-05/tone equalizer.png" alt="Darktable tone equalizer" />
<figcaption>Darktable's basic controls</figcaption>
</figure>

<p>
In contrast, a similar workflow in Capture One happens entirely on one screen, just by going from one slider to the next:
</p>

<figure style="width: 100%; text-align: center;">
<img width="25%" src="/static/2021-05/basic controls.png" alt="Capture One basic controls" />
<figcaption>Capture One's basic controls</figcaption>
</figure>

<p>
Furthermore, common tools such as cropping, rotating, and the white balance picker are accessible at all times with highly memorable keyboard shortcuts (C, R, W, respectively). Taken as a whole, this allows me to positively blaze through images compared to my Darktable workflow. It took me a while to appreciate how significant this difference was in my use: I now sometimes do a few days’ edits in a spare half hour, which used to be an all-evening affair in my usual Darktable workflow.
</p>

<p>
Which is not entirely Darktable's fault. Assigning custom keyboard shortcuts in Darktable can speed things up, and of course Darktable’s module system is infinitely more powerful, so there's a reason for its complexity. But the above example highlights a bit of a philosophical difference between Darktable’s unflinching priority on user control, and Capture One’s compromise between power and speed. There are upsides and downsides to both, but at this moment in my life, I begrudgingly value speed over power.
</p>

<p>
However, there were also a number of occasions where I missed Darktable’s deep control. Most notably, Capture One’s High Dynamic Range sliders and Clarity controls feel a bit restrictive and oversimplified: It can be hard to control which parts of the image are affected, and the tools are prone to produce artifacts such as halos if not managed carefully. In contrast, Darktable's continuous Tone Equalizer mask gives very precise and adjustable control over the affected area. Similarly, Darktable's Contrast Equalizer can control local contrast at arbitrary wavelet sizes, not just the very small ("Structure") or very big ("Clarity") ones, for example for specifically highlighting tree trunks or bird feathers.
</p>

<figure style="width: 100%; text-align: center;">
<a href="/static/2021-05/darktable.png" target="_blank"><img width="75%" src="/static/2021-05/darktable.png" alt="Darktable develop screen with contrast equalizer" /></a>
<figcaption>Highlighting feather detail with darktable's contrast equalizer</figcaption>
</figure>

<p>
Another annoyance with Capture One can be found in colors at the edges of the tone curve: When pushing exposure, Capture One tends to turn brightly colored highlight towards primary colors such as cyan/magenta/yellow before desaturating them into white. Similarly, shadow recovery sometimes pushes brightness and saturation a bit more strongly for brightly colored shadows than for dull ones, leading to an unnatural glowy effect. These cases are usually easy enough to fix with a quick tug on the shadows/highlights slider or by reigning in the offending color's lightness in the Color Editor, but they smell a bit of a runaway algorithm, which bothers me. Although in direct comparison they bother me less than the various rocks and hard places in Darktable's Filmic RGB chrominance preservation modes.
</p>

<p>
On the topic of bothersome details, Capture One's repair layer seemingly has a mind of its own when deciding whether to create a new control point, or adding to an existing one. The Highlight and White slider in the High Dynamic Range module sometimes can't quite decide whether to move white point, and sometimes lead to lightness reversals around bright objects. More on the algorithmic side, it annoys me that only the Exposure and High Dynamic Range modules have access to the full dynamic range of the raw file, while all the other tools apparently come after the screen transform, and therefore can't reach beyond the black point and white point. This makes Levels and Curve a bit less useful than I'm used to. Demosaicing is also not <i>quite</i> as detailed as Darktable's, although only by a tiny margin.
</p>

<p>
One surprising limitation of Capture One is that its support for cameras and lenses is a bit hit-and-miss. Film simulations are only supported for my Fuji X-T2, but not for the older Fuji X100T, nor the Ricohs or Panasonic. Some of their lens profiles are also laughably bad, and leave obvious reverse-vignetting or barrel distortion when enabled. Frankly, Darktable does better in terms of lens support, despite relying solely on volunteer support.
</p>

<p>
In terms of UI, Capture One is generally well-organized and easily configurable. But there is a constant stumbling block in the Layers module that I find very annoying: many of Capture One's tools automatically create new layers, but deselecting the tool does not deselect the layer. As delightfully easy it is to press B for the brush and paint in some Clarity, as unnecessarily laborious it is to then spend three clicks to slide open the Layers module and select the background layer again to resume editing. Or alternatively, wonder why your edits don't work while you <i>don't</i> have the background layer selected. It's a true pain.
</p>

<figure style="width: 100%; text-align: center;">
<img width="25%" src="/static/2021-05/layers.png" alt="Capture One layers" />
<figcaption>With a layer selected, edits only affect the layer</figcaption>
</figure>

<p>
As the last point of the nit picking, I was disappointed by the number of minor technical bugs I encountered in my use of Capture One: Half the time, Capture One starts half zoomed-in, and leaves me scratching my head at what weird compositions I chose for a few seconds. And the main, zoomed-out view is weirdly blurry if you use two displays with different DPI. Both of these issues are well-documented on their forums for several releases, but have not been fixed. For the exorbitant price that Capture One commands, such issues and customer communication are frankly unaccetable.
</p>

<p>
Finally, a few words about the library module and file organization. At first glance, I hated Capture One’s library. You have to import every single directory manually (I organize my images in daily directories), all the edits go into a central catalogue and nowhere else, and the sidecar files contain no editing information. But then someone told me a much better way: Instead of using Capture One’s <i>Catalogue</i>, create a <i>Session</i>, but ignore all those pre-built input and output directories, as well as the import button, and instead simply navigate to any old directory on your computer with the sidebar file browser. This is clearly not how sessions are meant to be used. But it actually works reasonably well, and puts editing information in a subdirectory next to the raw file.
</p>

<p>
On the whole, I grew to quite like Capture One, despite its flaws, mostly for its streamlined user interface and speed of operation. In terms of image quality, I honestly didn’t see much difference between Darktable and Capture One. But perhaps I am not the most discerning of users, either, as my focus is not on crazy detail recovery or the more technical arts of macro or astro. When it comes to control, I occasionally felt restricted and, dare I say, patronized by Capture One. But the speed of operation and general good-enough-ness of the image quality are still hard to argue with. I just wish they fixed their UI bugs, and improved their algorithms a bit.
</p>
<div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-photography.html">photography</a>
<div class="post-date">27 Dec 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-12-27-ios-and-android.html">A review of iOS, from an Android user's perspective</a></h1>
<p>
My Pixel 2 phone received its final software update the other day, and its battery took the occasion to really show its age. So it was time for a new phone. Which is not a decision to make lightly, considering that I spend multiple hours per day using it. And this time, I was inclined to give the iPhone another go, mostly because of Google's gross lack of human decency in the last few years. So, after years of using various Android devices, I bought a used, red, iPhone SE (2020). I made this choice with some trepidations, as my last experience with iOS was an iPad 3 from 2012. These are my experiences.
</p>

<p>
As a seasoned Android user, my first impressions of iOS were a bit of a mess. There are things that swipe up from the bottom, swipe down from the top, to the left of the home screens, to the right of the home screens, and at various permutations of pressing/holding the home button. Everything animates, takes a long time, and moves the viewport around. Particularly annoying is the placement of the back button in the top left corner, i.e. the most-inconvenient place on the entire screen for the arguably most-important gesture of the entire UI<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>. A close second are the context menus that slide out from a long-pressed item, with the menu thus in a different position on the screen every time you long press anything. These things may be less flashy on Android, but are seriously simpler.
</p>

<p>
I also immediately missed Android's customizeable home screen, with freely-positionable app icons and a plethora of useful widgets. iOS is very restrictive in this regard, and seemingly for no good reason. Why is the list of all apps (right-of-homescreen) sorted arbitrarily into nonsensical folders instead of a plain list? Why are app widgets allowed, but only on that weird left-of-the-home-screen screen? Why can't I have a currently-playing widget for my podcast player, a weather radar, or my groceries list? Apparently, iOS 14 has a brand new API that does now allow Android-like widgets in iOS, but at the moment they were only available for Apple's own (useless) apps.
</p>

<p>
My second big stumbling block was the iOS on-screen keyboard. At first glance, I thought it a terribly clunky thing. Actions as simple as inserting a comma require multiple taps, and positioning the cursor seemed almost comically difficult. But then I discovered that the "123" button in the bottom left can be swiped instead of tapped, which makes commas and periods and hyphens available to a quick swipe. That is seriously cool, if slightly hampered by my accidentally activating Control Center instead of swiping from "123" a bit too often. And precise cursor positioning is hidden behind a long-press of the spacebar. Very cool indeed. With these gestures, the iOS on-screen keyboard is actually not bad at all.
</p>

<p>
Autocorrect seems capable as well, and multi-language aware (hear that, Android?). And, mind-blowingly, a triple-swipe on the content area engages undo and redo in the current text area. Albeit a nigh-undiscoverable gesture, this is miles better than Androids undo/redo system (there isn't one). Actually, Android text fields <i>do</i> support undo and redo if you press Ctrl-Z on the keyboard, it's just that no on-screen keyboard has a Ctrl key. This is my number one grievance with Android at the moment (although there are <a href="https://play.google.com/store/apps/details?id=com.catchingnow.undo&amp;hl=en_US&amp;gl=US">workarounds</a>).
</p>

<p>
As a summary to the keyboard situation, I came to respect the iOS keyboard. I still miss a German "ß" key, more control about the autocorrect system, and a more modern design. But it works reasonably well. Better than Android's stock keyboard for sure.
</p>

<p>
My second big hurdle with iOS was the camera. Just like my Pixel's camera, the iPhone takes perfectly acceptible photos. But everything else is just plain worse than on Android. It starts with the way you get access to the camera, either by swiping right on the lock screen, or by tapping the camera app on the home screen. On Android, you double-click the lock button to start the camera, then hit the volume button to take a photo. Not only is this significantly faster, it also requires no ungloved finger, and no look onto the screen. And it doesn't make that abominable shutter sound, either, that the iPhone requires (unless the entire phone is silenced or you take video).
</p>

<p>
When I take a picture with my phone, it is because I don't have a dedicated camera at hand. Considering the number of cameras I own, this is most likely to happen when I need a camera <i>fast</i>. Thus the speed from pants to picture is extremely important to me, and this is a clear victory for Android. And additionally, Pixel phones have supported stacked RAWs since the original Pixel<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>, and offer quite a comprehensive image editing suite right in the stock camera app. And I prefer the Pixel's relatively neutral rendering over the iPhone's oversharpened, over-denoised, waxy images. But that might be just me.
</p>

<p>
At this point, I had more or less given up on iOS, and bought a Pixel 4a, which is probably Android's closest competitor to the iPhone SE.
</p>

<p>
Beyond the camera, there were a number of annoyances with iOS that I found hard to get used to. Like the fingerprint reader on the SE being very unreliable for me (50% miss rate), and awkwardly requiring a distinct push to activate the home button, where the Pixel's is quicker and in a more convenient location. The home button is in fact not a button, but an immovable touch pad that fakes a button-like behavior with a little rumble effect when pressed uncomfortably hard. Pressing a hard surface really hard did not get comfortable to me, especially not when invoking the multi-tasking switcher with a double-press. And it's awkwardly placed at the very bottom of the device, where my thumb does not rest with any kind of force in normal usage. Another real problem is the lack of third-party browsers<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>, which just leaves you in the cold should a website not work in Safari, or should you dislike Safari's rendering or its very limited ad-blockers. I was particularly annoyed by Safari's context menu for links, which slowly slides out from the link's location, often extending far outside of the screen. Thus opening links in new tabs is just awkwardly slow and annoying in Safari, where Android's more utilitarian menu gets the job done much quicker. Although a phone with a bigger screen might mitigate this problem somewhat.
</p>

<p>
On the topic of animations in general, one of my favorite features in Android is the animation speed setting in the Developer settings. If you want to feel like you got a new, faster phone, just set the animation speed to 0.5x, and marvel at the newfound snappiness of everything.
</p>

<p>
Apps in general feel annoyingly restrictive on iOS. Where is a true Firefox, with addons? Where is a Youtube player that can play a YouTube video in the background while the screen is locked, or block ads (NewPipe is the bee's knees!). Or a file synchronization tool that can actually synchronize useful data outside of its own sandox? In fact, my original hopes for iOS were driven by my memory of the fabled higher quality apps available only on the App Store. Looking at some of my old favorites on the SE however, I was forced to take off those rose-tinted glasses. These apps might have been radical back in the day, but the world has moved on. I don't see a pervasive difference in app quality between iOS and Android any longer. Of course iOS apps do still cost a more money on average, and often can't be test-driven without paying. That two-hour free return policy on the Play Store is seriously genius.
</p>

<p>
Additionally, there were a number of odd problems with the iPhone SE that I found hard to make sense of. For example, apps were very frequently getting booted out of memory. So much so in fact, that often my RSS feeds would not refresh in the background, and podcasts would not download. Even though the iPhone SE sure does have a lot less memory than the Pixel 4a, I hadn't expected this to be an actual problem<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>. And the iPhone would frequently misunderstand a vertical swipe for a horizontal one, or initiate an edge swipe when my finger was still clearly on the screen; My bluetooth headphones sounded noticeably worse, with a strange clipping artifact that should not be there; Significantly worse cell reception and voice quality; Low contrast and tiny text on lock screen notifications; And only barely adequate battery life. And let's not even talk about the stupid Lightning cable, the (comparatively) laggy and tiny TFT screen, the lack of a podcast client I like, and my horrible experiences during the setup process.
</p>

<p>
So, in summary, the iPhone SE was not for me. Don't get me wrong, it's a nice enough phone, and probably enough of a smartphone for most people. But there were a number of issues with both its hardware and its software, where I found Android and the Pixel 4a plainly more productive. Which is surprising, as my mind still had iOS pegged as the premium alternative, that I would totally buy if I wasn't too cheap for it. But in actual use I was annoyed at its preference of form over function, with many a distracting animation and a number of glaring ergonomic mistakes. Well, another lesson learned. The only truly significant advantage of iOS remains its vastly superior software support story: The SE will probably receive software updates until 2025 or 2027, where the best-in-class Pixel 4a will definitely expire in 2023.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">in some places, a right swipe can be used instead of the back button. But the behavior is too inconsistent to be useful</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">what Apple calls "ProRAW", and only makes available in the iPhone 12. iOS does support ordinary RAW, but only in third-party camera apps, and anyway non-stacked RAW files are rather useless from a tiny phone sensor</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">there are different browsers, but they are all required to use Safari's web renderer.</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">I hear this might have been a bug in iOs 14.3? What this says about the software quality of iOS in general is even more troubling, however.</p></div></div>


</div>
</div><div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-computers.html">computers</a> <a href="https://bastibe.de/tag-ui.html">ui</a>
<div class="post-date">27 Oct 2020</div><h1 class="post-title"><a href="https://bastibe.de/2020-10-26-differences-between-cameras.html">Differences Between Camera Sensors</a></h1>
<p>
Most cameras have the option to capture <i>raw</i> images, i.e. un-processed image data right from the image sensor. In theory, these images are pure physical measurements of light, and should therefore be very comparable between cameras. But are they? To investigate, I took a <i>raw</i> picture of <a href="https://en.wikipedia.org/wiki/ColorChecker">a color target</a> with each of my five cameras, and compared their output.
</p>

<p>
Capturing accurate colors is a surprisingly intricate matter, as the color depends on the spectrum of the illumination, the reflective spectrum of the colored object, and the color filters in the camera. I normalized the illumination by taking pictures on an overcast day outside<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>, and used a standard color target with 24 colored patches of a certified color.
</p>

<p>
The resulting colored spectra are captured on the camera sensor by photon counters behind three color filters “red”, “green”, and “blue”<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>, which differ from camera to camera in their spectral sensitivity. The recorded colors then get projected on a computer screen with another set of “red”, “green”, and “blue” LEDs of some different spectral makeup, or printed in three or more inks on paper. And this is then seen with eyes of yet another set of “red”, “green”, and “blue” retinal cells. Let's leave it at <i>color is complicated</i>.
</p>

<p>
At any rate, I took pictures at base ISO in <i>raw</i>, white-balanced and exposed for the third grey patch, and adjusted in saturation on the red patch. Here are the colors on the color checker and the recorded colors of my cameras:
</p>

<img style="width:70%;padding-left:15%;padding-top:5px;padding-bottom:10px" src="/static/2020-10/Colorchecker.svg">

<p>
The image shows the 24 colored patches of the color target<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>. Within each patch, five rectangles show the recorded color of (in reading order) the Pentax Q7, the Ricoh GR, the Panasonic LX100, the Fujifilm X-T2, and the Google Pixel 2.
</p>

<p>
Due to my exposure and white balance calibration, the third grey patch is completely equalized and shows no difference between the cameras and the target. The “Light Skin” patch (second on the first row) is also very close to equal, which is an important sanity check for my measurement procedure, as skin tones are optimized by all cameras. Seeing that the skin color patch is one of the most similar hopefully indicates that I did not do anything gravely wrong.
</p>

<p>
The last row shows greys, which are very similar across cameras (excluding the white patch for now). This indicates that RAW values indeed correspond to photon counts, with little additional processing. I have no idea why the white patch is off. Perhaps due to specular reflections on the white patch? Color is complicated.
</p>

<p>
As for the other colors, the Panasonic LX100 (center) and the Fujifilm X-T2 (bottom left) seem to record somewhat more accurate colors than the other three cameras. The Pentax Q7 (top left) and Ricoh GR (top right) seem comparatively weak across the board. The Panasonic (center) is great in darker blues and reds and greens, but somewhat weaker in yellows and pastels. The Fujifilm (bottom left) is very good in almost all colors, with pastels somewhat weaker than darker colors. The Google Pixel 2 (bottom right) is better in reds and greens and yellows than in blues and oranges. Pastel colors in general seem weaker than darker colors, which might be due to the same process that affected the white patch as well.
</p>

<p>
I think I learned something from this experiment: There are indeed color differences between cameras even when shooting RAW, but they are relatively minor. Whenever I see larger differences in my pictures, they are likely (easily correctible) white balance differences, and not sensor differences. When in doubt however, my Panasonic LX100 and Fujifilm X-T2 do capture more lifelike colors than my other cameras.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">any kind of artificial light is comparatively terrible for color reproduction!</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">in quotes, because colors are three-channel information, and we're talking about single channels here.</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">encoded as sRGB, which might or might not be displayed correctly on your screen, depending on your OS, your browser, your settings, your room's illumination, and your screen. Color is complicated.</p></div></div>


</div>
</div><div class="taglist"><a href="https://bastibe.de/tags.html">Tags</a>: <a href="https://bastibe.de/tag-photography.html">photography</a> <div id="archive">
<a href="https://bastibe.de/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status"><center><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br /><span xmlns:dct="https://purl.org/dc/terms/" href="https://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">bastibe.de</span> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://bastibe.de" property="cc:attributionName" rel="cc:attributionURL">Bastian Bechtold</a> is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</center></div>
</body>
</html>
